def predict(text):
  model_inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
  outputs = model.generate(
      model_inputs["input_ids"].to(model.device),
      attention_mask=model_inputs["attention_mask"].to(model.device),
      max_new_tokens=200,
      length_penalty=10,
      num_beams=12,
      early_stopping=3,
      num_return_sequences=3)
  return tokenizer.batch_decode(outputs)