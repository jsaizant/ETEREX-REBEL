def predict(text):
  model_inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
  outputs = model.generate(
      model_inputs["input_ids"].to(model.device),
      attention_mask=model_inputs["attention_mask"].to(model.device),
      max_new_tokens=200,
      length_penalty=1,
      num_beams=6,
      do_sample=True,
      top_p=0.98,
      top_k=0,
      num_return_sequences=2)
  return tokenizer.batch_decode(outputs)